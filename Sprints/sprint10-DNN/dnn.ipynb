{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "<h2>[Problem 1] Classifying fully connected layers</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      Number of nodes in the previous layer\n",
    "    n_nodes2 : int\n",
    "      Number of nodes in the later layer\n",
    "    initializer: instance of initialization method\n",
    "    optimizer: instance of optimization method\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # Initialize\n",
    "        # Initialize self.W and self.B using the initializer method\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
    "            output\n",
    "        \"\"\"       \n",
    "        self.X = X\n",
    "        A = X @ self.W + self.B\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient flowing from behind\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
    "            Gradient to flow forward\n",
    "        \"\"\"\n",
    "        self.dA = dA\n",
    "        dZ = dA @ self.W.T\n",
    "        # update\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "source": [
    "<h2>[Problem 2] Classifying the initialization method</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    Simple initialization with Gaussian distribution\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          Number of nodes in the previous layer\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer\n",
    "        Returns\n",
    "        ----------\n",
    "        W : weight\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer\n",
    "        Returns\n",
    "        ----------\n",
    "        B : bias\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.randn(n_nodes2)"
   ]
  },
  {
   "source": [
    "<h2>[Problem 3] Classifying optimization methods</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : Learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases for a layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Instance of the layer before update\n",
    "        \"\"\"\n",
    "        dW = layer.X.T @ layer.dA\n",
    "        dB = np.sum(layer.dA, axis=0)\n",
    "        layer.W -= self.lr * dW\n",
    "        layer.B -= self.lr * dB\n",
    "        return layer"
   ]
  },
  {
   "source": [
    "<h2>[Problem 4] Classifying activation functions</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, A):\n",
    "        \"\"\"return Z\"\"\"\n",
    "        return np.tanh(A)\n",
    "    \n",
    "    def backward(self, dZ, Z):\n",
    "        \"\"\"return dA\"\"\"\n",
    "        return dZ * (1 - Z**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self, A):\n",
    "        return 1 / (1 + np.exp(-A))\n",
    "    \n",
    "    def backward(self, dZ, Z):\n",
    "        return dZ * ((1 - Z) * Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, A):\n",
    "        return np.exp(A) / np.sum(np.exp(A), axis=1).reshape(-1, 1)\n",
    "    \n",
    "    def backward(self, Z, y):\n",
    "        \"\"\"return dA, CEE\"\"\"\n",
    "        return (Z - y) / y.shape[0], -np.sum((y * np.log(Z + 1e-7))) / y.shape[0]"
   ]
  },
  {
   "source": [
    "<h2>[Problem 5] ReLU class creation</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, A):\n",
    "        return np.maximum(A, 0)\n",
    "    \n",
    "    def backward(self, Z):\n",
    "        return np.where(Z > 0, 1, 0)"
   ]
  },
  {
   "source": [
    "<h2>[Problem 6] Initial value of weight</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    def __init__(self, n_nodes1):\n",
    "        self.sigma = 1.0 / n_nodes1**(1/2)\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        return self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        return self.sigma * np.random.randn(n_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    def __init__(self, n_nodes1):\n",
    "        self.sigma = (2.0 / n_nodes1)**(1/2)\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        return self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        return self.sigma * np.random.randn(n_nodes2)"
   ]
  },
  {
   "source": [
    "<h2>[Problem 7] Optimization method</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.H = 0\n",
    "\n",
    "    def update(self, layer):\n",
    "        dW = layer.X.T @ layer.dA\n",
    "        self.H += dW**2\n",
    "        layer.W -= (self.lr / self.H**(1/2)) * dW\n",
    "        return layer"
   ]
  },
  {
   "source": [
    "<h2>[Problem 8] Class completion</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "Iterator to get a mini-batch\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "      Training data\n",
    "    y : The following form of ndarray, shape (n_samples, 1)\n",
    "      Correct answer value\n",
    "    batch_size : int\n",
    "      Batch size\n",
    "    seed : int\n",
    "      NumPy random number seed\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]   \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "    \"\"\"\n",
    "    Simple three-layer neural network classifier\n",
    "    Parameters\n",
    "    ----------\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                hidden_layers=[400,200],\n",
    "                hidden_activations=['tanh', 'tanh'],\n",
    "                hidden_initializers=['gaussian', 'gaussian'],\n",
    "                sigma=0.01,\n",
    "                optimizer='sgd',\n",
    "                layer_type=None,\n",
    "                batch_size=20,\n",
    "                epochs=10,\n",
    "                lr=0.01,\n",
    "                seed=0,\n",
    "                verbose=False\n",
    "                ):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activations_name = hidden_activations.copy()\n",
    "        self.activations_name.insert(0, None)\n",
    "        self.activations_name.append('softmax')\n",
    "        self.initializers = hidden_activations.copy()\n",
    "        self.initializers.insert(0, None)\n",
    "        self.initializers.append('gaussian')\n",
    "        self.sigma = sigma\n",
    "        self.optimizer = optimizer\n",
    "        self.layer_type = layer_type\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.seed = seed\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _initialize_neural_network(self):\n",
    "        #choose optimizer \n",
    "        if (self.optimizer == 'sgd'):\n",
    "            self.optimizer = SGD(self.lr)\n",
    "        elif (self.optimizer == 'adagrad'):\n",
    "            self.optimizer = AdaGrad(self.lr)\n",
    "        #choose activations\n",
    "        self.activations = np.full(len(self.n_nodes), None) #activations[0] unused\n",
    "        for i in range(1, len(self.n_nodes)):\n",
    "            if (self.activations_name[i] == 'tanh'):\n",
    "                self.activations[i] = Tanh()\n",
    "            elif (self.activations_name[i] == 'sigmoid'):\n",
    "                self.activations[i] = Sigmoid()\n",
    "            elif (self.activations_name[i] == 'relu'):\n",
    "                self.activations[i] = ReLU()\n",
    "            elif (self.activations_name[i] == 'softmax'):\n",
    "                self.activations[i] = Softmax()\n",
    "        #choose initializer and build neural network\n",
    "        self.FC = np.full(len(self.n_nodes), None) #FC[0] unused\n",
    "        for i in range(1, len(self.n_nodes)):\n",
    "            if (self.initializers[i] == 'gaussian'):\n",
    "                self.FC[i] = FC(self.n_nodes[i - 1], self.n_nodes[i], SimpleInitializer(self.sigma), self.optimizer)\n",
    "            elif (self.initializers[i] == 'xavier'):\n",
    "                self.FC[i] = FC(self.n_nodes[i - 1], self.n_nodes[i]. XavierInitializer(self.n_nodes[i - 1]), self.optimizer)\n",
    "            elif (self.initializers[i] == 'he'):   \n",
    "                self.FC[i] = FC(self.n_nodes[i - 1], self.n_nodes[i]. HeInitializer(self.n_nodes[i - 1]), self.optimizer)\n",
    "        self.A = np.full(len(self.n_nodes), None) #A[0] unused\n",
    "        self.Z = np.full(len(self.n_nodes), None)\n",
    "        self.Z[0] = X\n",
    "        self.dA = np.full(len(self.n_nodes), None) #A[0] unused\n",
    "        self.dZ = np.full(len(self.n_nodes), None) #Z[-1] unused\n",
    "        self.CEE_list = []\n",
    "\n",
    "    def _forward(self, X):\n",
    "        for i in range(1, len(self.n_nodes)):\n",
    "            self.A[i] = self.FC[i].forward(self.Z[i - 1])\n",
    "            self.Z[i] = self.activations[i].forward(self.A[i])\n",
    "\n",
    "    def _forward_predict(self, X):\n",
    "        Z_pred[0] = X\n",
    "        for i in range(1, len(self.n_nodes)):\n",
    "            A_pred[i] = self.FC[i].forward(Z_pred[i - 1])\n",
    "            Z_pred[i] = self.activations[i].forward(A_pred[i])\n",
    "        return Z_pred\n",
    "\n",
    "    def _backward(self, y):\n",
    "        for i in range(len(self.n_nodes) - 1, 0, -1):\n",
    "            if (self.activations_name[i] == 'tanh' or self.activations_name[i] == 'sigmoid'):\n",
    "                self.dA[i] = self.activations[i].backward(self.dZ, self.Z[i])\n",
    "            elif (self.activations_name[i] == 'relu'):\n",
    "                self.dA[i] = self.activations[i].backward(self.Z[i])\n",
    "            elif (self.activations_name[i] == 'softmax'):\n",
    "                self.dA[i], CEE = self.activations[i].backward(self.Z[i], y)\n",
    "                self.CEE_list.append(CEE)\n",
    "            self.dZ[i - 1] = self.FC[i].backward(self.dA[i])\n",
    "\n",
    "    def _optimize(self, X, y):\n",
    "        get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=self.seed)\n",
    "        for _ in range(0, self.epochs):\n",
    "            for mini_X, mini_y in get_mini_batch:\n",
    "                self._forward(mini_X)\n",
    "                self._backward(mini_y) \n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Learn a neural network classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of training data\n",
    "        y : The following form of ndarray, shape (n_samples,)\n",
    "            Correct answer value of training data\n",
    "        X_val : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of verification data\n",
    "        y_val : The following form of ndarray, shape (n_samples,)\n",
    "            Correct value of verification data\n",
    "        \"\"\"\n",
    "        self.n_output = y.shape[1]\n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_nodes = self.hidden_layers.copy()\n",
    "        self.n_nodes.insert(0, self.n_features)\n",
    "        self.n_nodes.append(self.n_output)\n",
    "        self._initialize_neural_network()\n",
    "        self._optimize(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Estimate using a neural network classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            sample\n",
    "        Returns\n",
    "        -------\n",
    "            The following form of ndarray, shape (n_samples, 1)\n",
    "            Estimated result\n",
    "        \"\"\"\n",
    "        Z_pred = self._forward_predict(X)\n",
    "        return np.argmax(Z_pred[-1], axis=1), Z_pred[-1]"
   ]
  },
  {
   "source": [
    "<h2>[Problem 9] Learning and estimation</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}