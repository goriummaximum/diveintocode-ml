{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavier Initializer\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : number of nodes in the first layer / number of channels\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1):\n",
    "        self.sigma = 1.0 / n_nodes1**(1/2)\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          Number of nodes in the previous layer / number of channels\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer / number of features\n",
    "        Returns\n",
    "        ----------\n",
    "        W : weight\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer / number of features\n",
    "        Returns\n",
    "        ----------\n",
    "        B : bias\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.randn(n_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    def __init__(self, n_nodes1):\n",
    "        self.sigma = (2.0 / n_nodes1)**(1/2)\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        return self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        return self.sigma * np.random.randn(n_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, A):\n",
    "        return np.maximum(A, 0)\n",
    "    \n",
    "    def backward(self, dZ, Z):\n",
    "        return dZ * (Z > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : Learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases for a layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Instance of the layer before update\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>[Problem 1] Creating a one-dimensional convolutional layer class that limits the number of channels to one</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d:\n",
    "    def __init__(self, F, initializer, optimizer):\n",
    "        self.F = F\n",
    "        self.optimizer = optimizer\n",
    "        self.W = np.squeeze(initializer.W(1, F))\n",
    "        self.B = np.squeeze(initializer.B(1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward propagation, finding A\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (features, )\n",
    "        Return\n",
    "        ----------\n",
    "        A : (n_out, )\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        A = np.empty(compute_output_size(len(X), 0, self.F, 1))\n",
    "        for i in np.arange(0, len(A)):\n",
    "            A[i] = np.dot(X[i:i+self.F], self.W) + self.B\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        #update dW\n",
    "        self.dW = np.empty(self.F)\n",
    "        for s in np.arange(0, self.F):\n",
    "            self.dW[s] = np.dot(dA, self.X[s:s+len(dA)])\n",
    "        #update dB\n",
    "        self.dB = np.sum(dA)\n",
    "        #update dZ\n",
    "        self.dZ = np.empty(len(self.X))\n",
    "        for j in np.arange(0, len(self.X)):\n",
    "            total = 0.0\n",
    "            for s in np.arange(0, self.F):\n",
    "                if (j - s >= 0 and j - s < len(dA)):\n",
    "                    total += dA[j - s] * self.W[s]\n",
    "            self.dZ[j] = total\n",
    "        # update new W, B\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>[Problem 2] Output size calculation after one-dimensional convolution</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output_size(n_in, P, F, S):\n",
    "    return int(1 + (n_in + 2*P - F) / S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>[Problem 3] Experiment of one-dimensional convolutional layer with small array</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Forward</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4]).astype(np.float64)\n",
    "w = np.array([3, 5, 7]).astype(np.float64)\n",
    "b = np.array([1]).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplec1d = SimpleConv1d(len(w), XavierInitializer(len(x)), SGD())\n",
    "simplec1d.B = b\n",
    "simplec1d.W = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [35. 50.]\n"
     ]
    }
   ],
   "source": [
    "A = simplec1d.forward(x)\n",
    "print(\"A: {}\".format(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Backward</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW: [ 50.  80. 110.]\n",
      "dB: 30.0\n",
      "dZ: [ 30. 110. 170. 140.]\n"
     ]
    }
   ],
   "source": [
    "dA = np.array([10, 20]).astype(np.float64)\n",
    "simplec1d.backward(dA)\n",
    "print(\"dW: {}\\ndB: {}\\ndZ: {}\".format(simplec1d.dW, simplec1d.dB, simplec1d.dZ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>[Problem 4] Creating a one-dimensional convolutional layer class that does not limit the number of channels</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    A class of a convolution 1D layer\n",
    "    Parameters\n",
    "    ----------\n",
    "    F : int\n",
    "      filter size\n",
    "    ch_in : int\n",
    "      input channels\n",
    "    ch_in : int\n",
    "      output channels\n",
    "    initializer: instance of initialization method\n",
    "    optimizer: instance of optimization method\n",
    "    \"\"\"\n",
    "    def __init__(self, F, ch_in, ch_out, initializer, optimizer):\n",
    "        self.F = F\n",
    "        self.ch_in = ch_in\n",
    "        self.ch_out = ch_out\n",
    "        self.optimizer = optimizer\n",
    "        self.W = np.empty((ch_out, ch_in, F))\n",
    "        for ch in np.arange(0, ch_out):\n",
    "            self.W[ch] = initializer.W(ch_in, F)\n",
    "        self.B = initializer.B(ch_out)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward propagation, finding A\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (ch_in, number of features)\n",
    "        Return\n",
    "        ----------\n",
    "        A : (number of output channels, n_out)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.n_out = compute_output_size(self.X.shape[1], 0, self.F, 1)\n",
    "        A = np.empty((self.ch_out, self.n_out))\n",
    "        for ch in np.arange(0, self.ch_out):\n",
    "            for i in np.arange(0, self.n_out):\n",
    "                A[ch, i] = np.sum(X[:, i:i+self.F] * self.W[ch]) + self.B[ch]\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : The following forms of ndarray, shape (ch_out, n_out)\n",
    "            Gradient flowing from behind\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : The following forms of ndarray, shape (ch_in, number of features)\n",
    "            Gradient to flow forward\n",
    "        \"\"\"\n",
    "        #update dW\n",
    "        self.dW = np.empty((self.ch_out, self.ch_in, self.F))\n",
    "        for c_out in np.arange(0, self.ch_out):\n",
    "            for c_in in np.arange(0, self.ch_in):\n",
    "                for s in np.arange(0, self.F):\n",
    "                    self.dW[c_out, c_in, s] = np.sum(dA[c_out] * self.X[c_in, s:s+self.n_out])\n",
    "        #update dB\n",
    "        self.dB = np.sum(dA, axis=1)\n",
    "        #update dZ\n",
    "        self.dZ = np.empty((self.ch_in, self.X.shape[1]))\n",
    "        for c_in in np.arange(0, self.ch_in):\n",
    "            for j in np.arange(0, self.X.shape[1]):\n",
    "                total = 0.0\n",
    "                for c_out in np.arange(0, self.ch_out):\n",
    "                    for s in np.arange(0, self.F):\n",
    "                        if (j - s >= 0 and j - s < self.n_out):\n",
    "                            total += dA[c_out, j - s] * self.W[c_out, c_in, s]\n",
    "                self.dZ[c_in, j] = total\n",
    "        # update new W, B\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Testcase 1</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Forward</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]).astype(np.float64) #shape (2, 4), (number of input channels, number of features).\n",
    "w = np.ones((3, 2, 3)).astype(np.float64) # Set to 1 for simplification of the example. (Number of output channels, number of input channels, filter size).\n",
    "b = np.array([1, 2, 3]).astype(np.float64) # (Number of output channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "con1d = Conv1d(w.shape[2], w.shape[1], w.shape[0], XavierInitializer(x.shape[0]), SGD())\n",
    "con1d.W = w\n",
    "con1d.B = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: \n",
      "[[16. 22.]\n",
      " [17. 23.]\n",
      " [18. 24.]]\n"
     ]
    }
   ],
   "source": [
    "A = con1d.forward(x)\n",
    "print(\"A: \\n{}\".format(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Backward</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW: \n",
      "[[[ 32.  53.  74.]\n",
      "  [ 53.  74.  95.]]\n",
      "\n",
      " [[ 80. 130. 180.]\n",
      "  [130. 180. 230.]]\n",
      "\n",
      " [[ 77. 133. 189.]\n",
      "  [133. 189. 245.]]]\n",
      "dB: \n",
      "[21. 50. 56.]\n",
      "dZ: \n",
      "[[ 65. 127. 127.  62.]\n",
      " [ 65. 127. 127.  62.]]\n"
     ]
    }
   ],
   "source": [
    "dA = np.array([[10, 11], [20, 30], [35, 21]]).astype(np.float64)\n",
    "con1d.backward(dA)\n",
    "print(\"dW: \\n{}\\ndB: \\n{}\\ndZ: \\n{}\".format(con1d.dW, con1d.dB, con1d.dZ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Testcase 2</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Forward</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,2,3,4]]).astype(np.float64)\n",
    "w = np.array([[[3, 5, 7]]]).astype(np.float64)\n",
    "b = np.array([1]).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "con1d = Conv1d(w.shape[2], w.shape[1], w.shape[0], XavierInitializer(x.shape[0]), SGD())\n",
    "con1d.W = w\n",
    "con1d.B = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[35. 50.]]\n"
     ]
    }
   ],
   "source": [
    "A = con1d.forward(x)\n",
    "print(\"A: {}\".format(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Backward</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW: [[[ 50.  80. 110.]]]\n",
      "dB: [30.]\n",
      "dZ: [[ 30. 110. 170. 140.]]\n"
     ]
    }
   ],
   "source": [
    "dA = np.array([[10, 20]]).astype(np.float64)\n",
    "con1d.backward(dA)\n",
    "print(\"dW: {}\\ndB: {}\\ndZ: {}\".format(con1d.dW, con1d.dB, con1d.dZ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>[Problem 8] Learning and estimation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    Iterator to get a mini-batch\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "      Training data\n",
    "    y : The following form of ndarray, shape (n_samples, 1)\n",
    "      Correct answer value\n",
    "    batch_size : int\n",
    "      Batch size\n",
    "    seed : int\n",
    "      NumPy random number seed\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]   \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scratch1dCNNClassifier:\n",
    "    \"\"\"\n",
    "    Simple three-layer neural network classifier\n",
    "    Parameters\n",
    "    ----------\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                hidden_layers=[400,200],\n",
    "                hidden_activations=['tanh', 'tanh'],\n",
    "                hidden_initializers=['gaussian', 'gaussian'],\n",
    "                sigma=0.01,\n",
    "                optimizer='sgd',\n",
    "                layer_type=None,\n",
    "                batch_size=20,\n",
    "                epochs=10,\n",
    "                lr=0.01,\n",
    "                seed=0,\n",
    "                verbose=False\n",
    "                ):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activations_name = hidden_activations.copy()\n",
    "        self.activations_name.insert(0, None)\n",
    "        self.activations_name.append('softmax')\n",
    "        self.initializers = hidden_initializers.copy()\n",
    "        self.initializers.insert(0, None)\n",
    "        self.initializers.append('gaussian')\n",
    "        self.sigma = sigma\n",
    "        self.optimizer = optimizer\n",
    "        self.layer_type = layer_type\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.seed = seed\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _initialize_neural_network(self):\n",
    "        #choose activations\n",
    "        self.activations = np.full(len(self.n_nodes), None) #activations[0] unused\n",
    "        for i in range(1, len(self.n_nodes)):\n",
    "            if (self.activations_name[i] == 'tanh'):\n",
    "                self.activations[i] = Tanh()\n",
    "            elif (self.activations_name[i] == 'sigmoid'):\n",
    "                self.activations[i] = Sigmoid()\n",
    "            elif (self.activations_name[i] == 'relu'):\n",
    "                self.activations[i] = ReLU()\n",
    "            elif (self.activations_name[i] == 'softmax'):\n",
    "                self.activations[i] = Softmax()\n",
    "        #choose initializer, optimizer and build neural network\n",
    "        self.FC = np.full(len(self.n_nodes), None) #FC[0] unused\n",
    "        for i in range(1, len(self.n_nodes)):\n",
    "            if (self.optimizer == 'sgd'):\n",
    "                if (self.initializers[i] == 'gaussian'):\n",
    "                    self.FC[i] = FC(self.n_nodes[i - 1], self.n_nodes[i], SimpleInitializer(self.sigma), SGD(self.lr))\n",
    "                elif (self.initializers[i] == 'xavier'):\n",
    "                    self.FC[i] = FC(self.n_nodes[i - 1], self.n_nodes[i], XavierInitializer(self.n_nodes[i - 1]), SGD(self.lr))\n",
    "                elif (self.initializers[i] == 'he'):   \n",
    "                    self.FC[i] = FC(self.n_nodes[i - 1], self.n_nodes[i], HeInitializer(self.n_nodes[i - 1]), SGD(self.lr))\n",
    "            elif (self.optimizer == 'adagrad'):\n",
    "                if (self.initializers[i] == 'gaussian'):\n",
    "                    self.FC[i] = FC(self.n_nodes[i - 1], self.n_nodes[i], SimpleInitializer(self.sigma), AdaGrad(self.lr, (self.n_nodes[i - 1], self.n_nodes[i])))\n",
    "                elif (self.initializers[i] == 'xavier'):\n",
    "                    self.FC[i] = FC(self.n_nodes[i - 1], self.n_nodes[i], XavierInitializer(self.n_nodes[i - 1]), AdaGrad(self.lr, (self.n_nodes[i - 1], self.n_nodes[i])))\n",
    "                elif (self.initializers[i] == 'he'):   \n",
    "                    self.FC[i] = FC(self.n_nodes[i - 1], self.n_nodes[i], HeInitializer(self.n_nodes[i - 1]), AdaGrad(self.lr, (self.n_nodes[i - 1], self.n_nodes[i])))\n",
    "\n",
    "    def _forward(self, X):\n",
    "        A = np.full(len(self.n_nodes), None) #A[0] unused\n",
    "        Z = np.full(len(self.n_nodes), None)\n",
    "        Z[0] = X\n",
    "        for i in range(1, len(self.n_nodes)):\n",
    "            A[i] = self.FC[i].forward(Z[i - 1])\n",
    "            Z[i] = self.activations[i].forward(A[i])\n",
    "        return Z\n",
    "\n",
    "    def _backward(self, y, Z):\n",
    "        dA = np.full(len(self.n_nodes), None) #A[0] unused\n",
    "        dZ = np.full(len(self.n_nodes), None) #Z[-1] unused\n",
    "        CEE = None\n",
    "        for i in range(len(self.n_nodes) - 1, 0, -1):\n",
    "            if (self.activations_name[i] == 'softmax'):\n",
    "                dA[i], CEE = self.activations[i].backward(Z[i], y)\n",
    "            else:\n",
    "                dA[i] = self.activations[i].backward(dZ[i], Z[i])\n",
    "            dZ[i - 1] = self.FC[i].backward(dA[i])\n",
    "        return CEE\n",
    "\n",
    "    def _compute_CEE(self, y, Z):\n",
    "        return -np.sum((y * np.log(Z + 1e-7))) / y.shape[0]\n",
    "\n",
    "    def _optimize(self, X, y, X_val=None, y_val=None):\n",
    "        self.CEE_list = []\n",
    "        self.CEE_list_val = []\n",
    "        get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=self.seed)\n",
    "        for _ in range(0, self.epochs):\n",
    "            for mini_X, mini_y in get_mini_batch:\n",
    "                Z = self._forward(mini_X)\n",
    "                CEE = self._backward(mini_y, Z)\n",
    "                self.CEE_list.append(CEE)\n",
    "            if (X_val is not None and y_val is not None):\n",
    "                Z = self._forward(X_val)\n",
    "                self.CEE_list_val.append(self._compute_CEE(y_val, Z[-1]))\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Learn a neural network classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of training data\n",
    "        y : The following form of ndarray, shape (n_samples,)\n",
    "            Correct answer value of training data\n",
    "        X_val : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of verification data\n",
    "        y_val : The following form of ndarray, shape (n_samples,)\n",
    "            Correct value of verification data\n",
    "        \"\"\"\n",
    "        self.n_output = y.shape[1]\n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_nodes = self.hidden_layers.copy()\n",
    "        self.n_nodes.insert(0, self.n_features)\n",
    "        self.n_nodes.append(self.n_output)\n",
    "        self._initialize_neural_network()\n",
    "        self._optimize(X, y, X_val, y_val)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Estimate using a neural network classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            sample\n",
    "        Returns\n",
    "        -------\n",
    "            The following form of ndarray, shape (n_samples, 1)\n",
    "            Estimated result\n",
    "        \"\"\"\n",
    "        Z_pred = self._forward(X)\n",
    "        return np.argmax(Z_pred[-1], axis=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf3ec189d6e56d1f0202dd4e4268996a952453029533e556245f105ea2c9a45f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}