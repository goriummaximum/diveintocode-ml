{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sprint13-tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7v2yB9kP03O"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNPYSGYyjNXj"
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcJp5P80ffMj"
      },
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDRaHKiHnUSR"
      },
      "source": [
        "iris_path = \"/content/drive/MyDrive/Colab Notebooks/data/Iris.csv\"\n",
        "house_path = \"/content/drive/MyDrive/Colab-Notebooks/data/house-price/train.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQGsKjCXuUj6"
      },
      "source": [
        "<h1>------- Introduction -------</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wmh94IEubDS"
      },
      "source": [
        "<h3>intro 1: data flow</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRMvffk2Qw0D",
        "outputId": "4fd39163-1513-43f6-c602-a010a2a677ef"
      },
      "source": [
        "a = tf.placeholder(tf.int32)\n",
        "b = tf.placeholder(tf.int32)\n",
        "add = tf.add(a, b)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    output = sess.run(add, feed_dict={a:5, b:-10.5})\n",
        "    print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoYBl8Dhuiiq"
      },
      "source": [
        "<h3>intro 2: logistic regression</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dS9Z5brZuTMX",
        "outputId": "16fbd958-4e95-48ba-c8e3-fb23c75e8051"
      },
      "source": [
        "x_train = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y_train = np.array([[0],[0],[0],[1]])\n",
        "\n",
        "print(x_train)\n",
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQpjU8EtvjOu"
      },
      "source": [
        "x = tf.placeholder(tf.float32, shape=[None, 2])\n",
        "t = tf.placeholder(tf.float32, shape=[None, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO1zFn3GyFVc"
      },
      "source": [
        "W = tf.Variable(tf.zeros(shape=[2,1]))\n",
        "b = tf.Variable(tf.zeros(shape=[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xAtCzk5zrH1"
      },
      "source": [
        "y = tf.sigmoid(tf.matmul(x, W) + b)\n",
        "cross_entropy = tf.reduce_sum(-t * tf.log(y) - (1 - t) * tf.log(1 - y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GSVH6ACwz2A"
      },
      "source": [
        "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPzS87anxqSY"
      },
      "source": [
        "#compare y_true (t) and y_pred(y)\n",
        "correct_prediction = tf.equal(tf.sign(y - 0.5), tf.sign(t - 0.5))\n",
        "#compute accuary\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC3T6t7B4Lyg",
        "outputId": "41d0cf46-b1bb-4593-8a1b-367f291c6f81"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch in range(1000):\n",
        "        sess.run(train_step, feed_dict={\n",
        "            x: x_train,\n",
        "            t: y_train\n",
        "        })\n",
        "\n",
        "        if (epoch % 100 == 0):\n",
        "            acc_val = sess.run(accuracy, feed_dict={\n",
        "                x: x_train,\n",
        "                t: y_train\n",
        "            })\n",
        "            print ('epoch: %d, Accuracy: %f'\n",
        "               %(epoch, acc_val))\n",
        "            print(\"W: {}    b: {}\".format(sess.run(W), sess.run(b)))\n",
        "            \n",
        "    print(\"y_proba; {}\".format(sess.run(y, feed_dict={x:x_train, t:y_train})))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, Accuracy: 0.750000\n",
            "W: [[0.]\n",
            " [0.]]    b: [-0.1]\n",
            "epoch: 100, Accuracy: 1.000000\n",
            "W: [[1.7671356]\n",
            " [1.7671356]]    b: [-2.9394417]\n",
            "epoch: 200, Accuracy: 1.000000\n",
            "W: [[2.702048]\n",
            " [2.702048]]    b: [-4.282485]\n",
            "epoch: 300, Accuracy: 1.000000\n",
            "W: [[3.345771]\n",
            " [3.345771]]    b: [-5.2272086]\n",
            "epoch: 400, Accuracy: 1.000000\n",
            "W: [[3.8412285]\n",
            " [3.8412285]]    b: [-5.9599466]\n",
            "epoch: 500, Accuracy: 1.000000\n",
            "W: [[4.244355]\n",
            " [4.244355]]    b: [-6.558406]\n",
            "epoch: 600, Accuracy: 1.000000\n",
            "W: [[4.5839767]\n",
            " [4.5839767]]    b: [-7.0637155]\n",
            "epoch: 700, Accuracy: 1.000000\n",
            "W: [[4.8771544]\n",
            " [4.8771544]]    b: [-7.5005603]\n",
            "epoch: 800, Accuracy: 1.000000\n",
            "W: [[5.1348853]\n",
            " [5.1348853]]    b: [-7.8849826]\n",
            "epoch: 900, Accuracy: 1.000000\n",
            "W: [[5.36469]\n",
            " [5.36469]]    b: [-8.228008]\n",
            "y_proba; [[1.9648671e-04]\n",
            " [4.9049795e-02]\n",
            " [4.9049795e-02]\n",
            " [9.3120372e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnQUzCSU4dMr"
      },
      "source": [
        "<h1>------- Sprint Tensorflow -------</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrdcRZg16Dfy"
      },
      "source": [
        "<h2>[Problem 1] Looking back on the scratch</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVPl8hJu7nyH"
      },
      "source": [
        "Step to implement deep learning\n",
        "<ol>\n",
        "    <li>preprocess input data X, y</li>\n",
        "    <li>choose activations functions</li>\n",
        "    <li>choose initializer, optimizer and initialize the neural network</li>\n",
        "    <li>iterate each epoch, get mini batch</li>\n",
        "    <li>do forward and backward, update W and b</li>\n",
        "    <li>update CEE</li>\n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuiHc7Yr88bV"
      },
      "source": [
        "<h2>[Problem 2] Consider the correspondence between scratch and TensorFlow</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOf67zBFP1PF"
      },
      "source": [
        "Load data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pKq3bykzCFJL",
        "outputId": "6b147ba6-9f24-43c0-fbf9-6ad234ff1d65"
      },
      "source": [
        "\"\"\"\n",
        "Binary classification of Iris dataset using neural network implemented in TensorFlow\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Load dataset\n",
        "df = pd.read_csv(iris_path)\n",
        "display(df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(150, 6)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XWFqx4PQcvD"
      },
      "source": [
        "Proprocessing X, y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "7cSVGNZsCKgC",
        "outputId": "8e6a10c9-c052-4a62-ac48-abee6c893c47"
      },
      "source": [
        "#Condition extraction from data frame\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "display(X.shape)\n",
        "display(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(100, 4)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vTNXTSjICSEV",
        "outputId": "a6e822a6-3e91-470f-c49b-f4ee832f2d58"
      },
      "source": [
        "# NumPy 配列に変換\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Convert label to number\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "display(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(100, 1)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLIryiYkCgus"
      },
      "source": [
        "#Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foqyzc0xCtwN"
      },
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get a mini-batch\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : The following form of ndarray, shape (n_samples, 1)\n",
        "      Correct answer value\n",
        "    batch_size : int\n",
        "      Batch size\n",
        "    seed : int\n",
        "      NumPy random number seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX9fbxBpQjLl"
      },
      "source": [
        "The main differences between Tensorflow implementation and my previous implementation of deep neural network are:\n",
        "<ol>\n",
        "<li>Tensorflow describe a strongly linking graph of calculation and objects via example_net(x).</li>\n",
        "<li>No more implementation of forward and backward in Tensorflow, Tensorflow automatically detects and updates crucial variables.</li>\n",
        "<li>Fewer lines of code.</li>\n",
        "<li>No more thinking about the order of code execution, just descriptions.</li>\n",
        "<li>No more worry about the shape of the array when calculating.</li>\n",
        "<li>Plug and play.</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTqNs8Ne7VsO",
        "outputId": "3b77511a-742b-48a2-cb48-c90745baeae3"
      },
      "source": [
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weight and bias\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    return layer_output\n",
        "#Read network structure                              \n",
        "logits = example_net(X)\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#Initialization of variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 7.0241, val_loss : 67.6860, acc : 0.375\n",
            "Epoch 1, loss : 3.4241, val_loss : 23.4026, acc : 0.312\n",
            "Epoch 2, loss : 1.9387, val_loss : 11.6681, acc : 0.375\n",
            "Epoch 3, loss : 2.0917, val_loss : 13.1400, acc : 0.312\n",
            "Epoch 4, loss : 1.7685, val_loss : 17.7284, acc : 0.312\n",
            "Epoch 5, loss : 1.6097, val_loss : 12.9607, acc : 0.312\n",
            "Epoch 6, loss : 1.4402, val_loss : 10.0593, acc : 0.312\n",
            "Epoch 7, loss : 1.3704, val_loss : 9.4797, acc : 0.312\n",
            "Epoch 8, loss : 1.2536, val_loss : 9.8518, acc : 0.312\n",
            "Epoch 9, loss : 1.1476, val_loss : 8.5670, acc : 0.375\n",
            "Epoch 10, loss : 1.0930, val_loss : 8.0430, acc : 0.375\n",
            "Epoch 11, loss : 1.0412, val_loss : 7.8791, acc : 0.375\n",
            "Epoch 12, loss : 0.9804, val_loss : 7.1233, acc : 0.375\n",
            "Epoch 13, loss : 0.9326, val_loss : 6.7908, acc : 0.375\n",
            "Epoch 14, loss : 0.8792, val_loss : 6.2492, acc : 0.375\n",
            "Epoch 15, loss : 0.8304, val_loss : 5.7681, acc : 0.375\n",
            "Epoch 16, loss : 0.7835, val_loss : 5.2886, acc : 0.438\n",
            "Epoch 17, loss : 0.7384, val_loss : 4.8037, acc : 0.438\n",
            "Epoch 18, loss : 0.6961, val_loss : 4.3575, acc : 0.500\n",
            "Epoch 19, loss : 0.6543, val_loss : 3.9175, acc : 0.500\n",
            "Epoch 20, loss : 0.6136, val_loss : 3.5188, acc : 0.500\n",
            "Epoch 21, loss : 0.5738, val_loss : 3.1371, acc : 0.500\n",
            "Epoch 22, loss : 0.5349, val_loss : 2.7697, acc : 0.500\n",
            "Epoch 23, loss : 0.4973, val_loss : 2.4528, acc : 0.562\n",
            "Epoch 24, loss : 0.4606, val_loss : 2.1728, acc : 0.562\n",
            "Epoch 25, loss : 0.4255, val_loss : 1.9324, acc : 0.625\n",
            "Epoch 26, loss : 0.3919, val_loss : 1.7049, acc : 0.625\n",
            "Epoch 27, loss : 0.3609, val_loss : 1.5248, acc : 0.688\n",
            "Epoch 28, loss : 0.3326, val_loss : 1.3725, acc : 0.750\n",
            "Epoch 29, loss : 0.3071, val_loss : 1.2386, acc : 0.750\n",
            "Epoch 30, loss : 0.2851, val_loss : 1.1453, acc : 0.750\n",
            "Epoch 31, loss : 0.2647, val_loss : 1.0364, acc : 0.750\n",
            "Epoch 32, loss : 0.2466, val_loss : 0.9368, acc : 0.750\n",
            "Epoch 33, loss : 0.2297, val_loss : 0.8304, acc : 0.750\n",
            "Epoch 34, loss : 0.2155, val_loss : 0.7243, acc : 0.750\n",
            "Epoch 35, loss : 0.2024, val_loss : 0.6215, acc : 0.750\n",
            "Epoch 36, loss : 0.1920, val_loss : 0.5405, acc : 0.812\n",
            "Epoch 37, loss : 0.1815, val_loss : 0.4516, acc : 0.812\n",
            "Epoch 38, loss : 0.1735, val_loss : 0.4015, acc : 0.812\n",
            "Epoch 39, loss : 0.1643, val_loss : 0.3323, acc : 0.812\n",
            "Epoch 40, loss : 0.1574, val_loss : 0.2935, acc : 0.875\n",
            "Epoch 41, loss : 0.1496, val_loss : 0.2519, acc : 0.875\n",
            "Epoch 42, loss : 0.1429, val_loss : 0.2206, acc : 0.875\n",
            "Epoch 43, loss : 0.1364, val_loss : 0.1932, acc : 0.875\n",
            "Epoch 44, loss : 0.1302, val_loss : 0.1684, acc : 0.875\n",
            "Epoch 45, loss : 0.1244, val_loss : 0.1464, acc : 0.875\n",
            "Epoch 46, loss : 0.1188, val_loss : 0.1264, acc : 0.875\n",
            "Epoch 47, loss : 0.1139, val_loss : 0.1098, acc : 0.875\n",
            "Epoch 48, loss : 0.1091, val_loss : 0.0947, acc : 0.938\n",
            "Epoch 49, loss : 0.1050, val_loss : 0.0833, acc : 0.938\n",
            "Epoch 50, loss : 0.1007, val_loss : 0.0712, acc : 1.000\n",
            "Epoch 51, loss : 0.0975, val_loss : 0.0653, acc : 1.000\n",
            "Epoch 52, loss : 0.0932, val_loss : 0.0531, acc : 1.000\n",
            "Epoch 53, loss : 0.0915, val_loss : 0.0557, acc : 1.000\n",
            "Epoch 54, loss : 0.0862, val_loss : 0.0390, acc : 1.000\n",
            "Epoch 55, loss : 0.0875, val_loss : 0.0598, acc : 0.938\n",
            "Epoch 56, loss : 0.0790, val_loss : 0.0376, acc : 1.000\n",
            "Epoch 57, loss : 0.0871, val_loss : 0.0935, acc : 0.938\n",
            "Epoch 58, loss : 0.0723, val_loss : 0.0862, acc : 0.938\n",
            "Epoch 59, loss : 0.0936, val_loss : 0.1731, acc : 0.938\n",
            "Epoch 60, loss : 0.0693, val_loss : 0.1648, acc : 0.938\n",
            "Epoch 61, loss : 0.1047, val_loss : 0.2714, acc : 0.938\n",
            "Epoch 62, loss : 0.0711, val_loss : 0.1687, acc : 0.938\n",
            "Epoch 63, loss : 0.1072, val_loss : 0.3122, acc : 0.938\n",
            "Epoch 64, loss : 0.0738, val_loss : 0.1538, acc : 0.938\n",
            "Epoch 65, loss : 0.1069, val_loss : 0.3194, acc : 0.938\n",
            "Epoch 66, loss : 0.0757, val_loss : 0.1524, acc : 0.938\n",
            "Epoch 67, loss : 0.1082, val_loss : 0.3200, acc : 0.938\n",
            "Epoch 68, loss : 0.0780, val_loss : 0.1584, acc : 0.938\n",
            "Epoch 69, loss : 0.1114, val_loss : 0.3153, acc : 0.938\n",
            "Epoch 70, loss : 0.0808, val_loss : 0.1600, acc : 0.938\n",
            "Epoch 71, loss : 0.1139, val_loss : 0.2888, acc : 0.938\n",
            "Epoch 72, loss : 0.0824, val_loss : 0.1424, acc : 0.938\n",
            "Epoch 73, loss : 0.1110, val_loss : 0.2300, acc : 0.938\n",
            "Epoch 74, loss : 0.0791, val_loss : 0.1061, acc : 0.938\n",
            "Epoch 75, loss : 0.1002, val_loss : 0.1768, acc : 0.938\n",
            "Epoch 76, loss : 0.0719, val_loss : 0.0579, acc : 0.938\n",
            "Epoch 77, loss : 0.0810, val_loss : 0.0948, acc : 0.938\n",
            "Epoch 78, loss : 0.0610, val_loss : 0.0222, acc : 1.000\n",
            "Epoch 79, loss : 0.0601, val_loss : 0.0222, acc : 1.000\n",
            "Epoch 80, loss : 0.0514, val_loss : 0.0117, acc : 1.000\n",
            "Epoch 81, loss : 0.0498, val_loss : 0.0060, acc : 1.000\n",
            "Epoch 82, loss : 0.0454, val_loss : 0.0173, acc : 1.000\n",
            "Epoch 83, loss : 0.0472, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 84, loss : 0.0430, val_loss : 0.0289, acc : 1.000\n",
            "Epoch 85, loss : 0.0449, val_loss : 0.0094, acc : 1.000\n",
            "Epoch 86, loss : 0.0400, val_loss : 0.0493, acc : 0.938\n",
            "Epoch 87, loss : 0.0430, val_loss : 0.0089, acc : 1.000\n",
            "Epoch 88, loss : 0.0374, val_loss : 0.0669, acc : 0.938\n",
            "Epoch 89, loss : 0.0429, val_loss : 0.0068, acc : 1.000\n",
            "Epoch 90, loss : 0.0361, val_loss : 0.0710, acc : 0.938\n",
            "Epoch 91, loss : 0.0422, val_loss : 0.0058, acc : 1.000\n",
            "Epoch 92, loss : 0.0349, val_loss : 0.0693, acc : 0.938\n",
            "Epoch 93, loss : 0.0411, val_loss : 0.0055, acc : 1.000\n",
            "Epoch 94, loss : 0.0337, val_loss : 0.0678, acc : 0.938\n",
            "Epoch 95, loss : 0.0397, val_loss : 0.0059, acc : 1.000\n",
            "Epoch 96, loss : 0.0326, val_loss : 0.0613, acc : 0.938\n",
            "Epoch 97, loss : 0.0376, val_loss : 0.0081, acc : 1.000\n",
            "Epoch 98, loss : 0.0316, val_loss : 0.0499, acc : 0.938\n",
            "Epoch 99, loss : 0.0349, val_loss : 0.0138, acc : 1.000\n",
            "test_acc : 0.900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4BAd6_PWbjd"
      },
      "source": [
        "<h2>[Problem 3] Create a model of Iris using all three types of objective variables</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOlGzRc2WbOY",
        "outputId": "01901da8-ec34-4474-8bb4-413a94bcb569"
      },
      "source": [
        "\"\"\"\n",
        "Trinary classification of Iris dataset using neural network implemented in TensorFlow\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Load dataset\n",
        "df = pd.read_csv(iris_path)\n",
        "df[\"Species\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Iris-setosa        50\n",
              "Iris-versicolor    50\n",
              "Iris-virginica     50\n",
              "Name: Species, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "l5tjmXr5ZNhA",
        "outputId": "ce69b827-539a-49e0-b008-319764c42f79"
      },
      "source": [
        "#Condition extraction from data frame\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "display(y.shape)\n",
        "display(X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(150,)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(150, 4)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "hbSNi77CZlsh",
        "outputId": "c4f1cc7f-4dd8-4d11-8827-ff18aab0b03a"
      },
      "source": [
        "# Convert label to one hot\n",
        "y = pd.get_dummies(y)\n",
        "\n",
        "# NumPy 配列に変換\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "y = y.astype(np.int64)\n",
        "display(y.shape)\n",
        "display(y[:5, :])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(150, 3)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN3vHWqnihm8"
      },
      "source": [
        "#Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PieFY9AFjK85",
        "outputId": "1f14d471-d784-46a2-d98d-21dd8757f564"
      },
      "source": [
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3\n",
        "\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weight and bias\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    return layer_output\n",
        "#Read network structure                              \n",
        "logits = example_net(X)\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.nn.softmax(logits) - 0.5))\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#Initialization of variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Epoch 0, loss : 11.3028, val_loss : 102.2222, acc : 0.528\n",
            "Epoch 1, loss : 8.7998, val_loss : 81.4813, acc : 0.583\n",
            "Epoch 2, loss : 6.8027, val_loss : 59.8930, acc : 0.694\n",
            "Epoch 3, loss : 4.7791, val_loss : 39.9069, acc : 0.694\n",
            "Epoch 4, loss : 3.0486, val_loss : 25.9002, acc : 0.472\n",
            "Epoch 5, loss : 1.6053, val_loss : 10.0990, acc : 0.639\n",
            "Epoch 6, loss : 0.4247, val_loss : 2.8406, acc : 0.806\n",
            "Epoch 7, loss : 0.0561, val_loss : 1.1800, acc : 0.861\n",
            "Epoch 8, loss : 0.0306, val_loss : 1.0144, acc : 0.917\n",
            "Epoch 9, loss : 0.0183, val_loss : 0.7311, acc : 0.944\n",
            "Epoch 10, loss : 0.0175, val_loss : 0.8111, acc : 0.944\n",
            "Epoch 11, loss : 0.0155, val_loss : 0.7300, acc : 0.944\n",
            "Epoch 12, loss : 0.0162, val_loss : 0.8065, acc : 0.944\n",
            "Epoch 13, loss : 0.0145, val_loss : 0.7146, acc : 0.944\n",
            "Epoch 14, loss : 0.0157, val_loss : 0.8387, acc : 0.944\n",
            "Epoch 15, loss : 0.0131, val_loss : 0.6572, acc : 0.944\n",
            "Epoch 16, loss : 0.0157, val_loss : 0.8889, acc : 0.944\n",
            "Epoch 17, loss : 0.0118, val_loss : 0.6051, acc : 0.889\n",
            "Epoch 18, loss : 0.0153, val_loss : 0.8688, acc : 0.944\n",
            "Epoch 19, loss : 0.0116, val_loss : 0.6202, acc : 0.944\n",
            "Epoch 20, loss : 0.0143, val_loss : 0.9097, acc : 0.944\n",
            "Epoch 21, loss : 0.0105, val_loss : 0.5645, acc : 0.889\n",
            "Epoch 22, loss : 0.0137, val_loss : 0.8264, acc : 0.944\n",
            "Epoch 23, loss : 0.0107, val_loss : 0.6522, acc : 0.944\n",
            "Epoch 24, loss : 0.0117, val_loss : 0.8235, acc : 0.944\n",
            "Epoch 25, loss : 0.0095, val_loss : 0.5577, acc : 0.917\n",
            "Epoch 26, loss : 0.0118, val_loss : 0.8524, acc : 0.944\n",
            "Epoch 27, loss : 0.0094, val_loss : 0.5415, acc : 0.917\n",
            "Epoch 28, loss : 0.0110, val_loss : 0.8188, acc : 0.944\n",
            "Epoch 29, loss : 0.0090, val_loss : 0.5910, acc : 0.944\n",
            "Epoch 30, loss : 0.0101, val_loss : 0.8106, acc : 0.944\n",
            "Epoch 31, loss : 0.0086, val_loss : 0.5879, acc : 0.944\n",
            "Epoch 32, loss : 0.0094, val_loss : 0.7882, acc : 0.944\n",
            "Epoch 33, loss : 0.0083, val_loss : 0.6213, acc : 0.944\n",
            "Epoch 34, loss : 0.0084, val_loss : 0.7378, acc : 0.944\n",
            "Epoch 35, loss : 0.0079, val_loss : 0.6718, acc : 0.944\n",
            "Epoch 36, loss : 0.0076, val_loss : 0.6888, acc : 0.944\n",
            "Epoch 37, loss : 0.0073, val_loss : 0.6835, acc : 0.944\n",
            "Epoch 38, loss : 0.0071, val_loss : 0.6920, acc : 0.944\n",
            "Epoch 39, loss : 0.0071, val_loss : 0.7064, acc : 0.944\n",
            "Epoch 40, loss : 0.0070, val_loss : 0.6946, acc : 0.944\n",
            "Epoch 41, loss : 0.0069, val_loss : 0.6993, acc : 0.944\n",
            "Epoch 42, loss : 0.0067, val_loss : 0.6907, acc : 0.944\n",
            "Epoch 43, loss : 0.0066, val_loss : 0.6927, acc : 0.944\n",
            "Epoch 44, loss : 0.0065, val_loss : 0.6860, acc : 0.944\n",
            "Epoch 45, loss : 0.0064, val_loss : 0.6825, acc : 0.944\n",
            "Epoch 46, loss : 0.0062, val_loss : 0.6765, acc : 0.944\n",
            "Epoch 47, loss : 0.0061, val_loss : 0.6714, acc : 0.944\n",
            "Epoch 48, loss : 0.0059, val_loss : 0.6650, acc : 0.944\n",
            "Epoch 49, loss : 0.0058, val_loss : 0.6595, acc : 0.944\n",
            "Epoch 50, loss : 0.0056, val_loss : 0.6535, acc : 0.944\n",
            "Epoch 51, loss : 0.0054, val_loss : 0.6491, acc : 0.944\n",
            "Epoch 52, loss : 0.0053, val_loss : 0.6433, acc : 0.944\n",
            "Epoch 53, loss : 0.0051, val_loss : 0.6388, acc : 0.944\n",
            "Epoch 54, loss : 0.0049, val_loss : 0.6330, acc : 0.944\n",
            "Epoch 55, loss : 0.0048, val_loss : 0.6284, acc : 0.944\n",
            "Epoch 56, loss : 0.0046, val_loss : 0.6230, acc : 0.944\n",
            "Epoch 57, loss : 0.0044, val_loss : 0.6183, acc : 0.944\n",
            "Epoch 58, loss : 0.0043, val_loss : 0.6133, acc : 0.944\n",
            "Epoch 59, loss : 0.0041, val_loss : 0.6086, acc : 0.944\n",
            "Epoch 60, loss : 0.0039, val_loss : 0.6040, acc : 0.944\n",
            "Epoch 61, loss : 0.0038, val_loss : 0.5994, acc : 0.944\n",
            "Epoch 62, loss : 0.0036, val_loss : 0.5949, acc : 0.944\n",
            "Epoch 63, loss : 0.0035, val_loss : 0.5907, acc : 0.944\n",
            "Epoch 64, loss : 0.0034, val_loss : 0.5829, acc : 0.944\n",
            "Epoch 65, loss : 0.0033, val_loss : 0.5940, acc : 0.944\n",
            "Epoch 66, loss : 0.0033, val_loss : 0.5821, acc : 0.944\n",
            "Epoch 67, loss : 0.0031, val_loss : 0.5734, acc : 0.944\n",
            "Epoch 68, loss : 0.0030, val_loss : 0.5729, acc : 0.944\n",
            "Epoch 69, loss : 0.0030, val_loss : 0.5712, acc : 0.944\n",
            "Epoch 70, loss : 0.0029, val_loss : 0.5684, acc : 0.944\n",
            "Epoch 71, loss : 0.0028, val_loss : 0.5690, acc : 0.944\n",
            "Epoch 72, loss : 0.0028, val_loss : 0.5659, acc : 0.944\n",
            "Epoch 73, loss : 0.0027, val_loss : 0.5625, acc : 0.944\n",
            "Epoch 74, loss : 0.0027, val_loss : 0.5606, acc : 0.944\n",
            "Epoch 75, loss : 0.0027, val_loss : 0.5690, acc : 0.944\n",
            "Epoch 76, loss : 0.0027, val_loss : 0.5603, acc : 0.944\n",
            "Epoch 77, loss : 0.0025, val_loss : 0.5527, acc : 0.944\n",
            "Epoch 78, loss : 0.0025, val_loss : 0.5537, acc : 0.944\n",
            "Epoch 79, loss : 0.0024, val_loss : 0.5546, acc : 0.944\n",
            "Epoch 80, loss : 0.0024, val_loss : 0.5537, acc : 0.944\n",
            "Epoch 81, loss : 0.0024, val_loss : 0.5498, acc : 0.944\n",
            "Epoch 82, loss : 0.0024, val_loss : 0.5539, acc : 0.944\n",
            "Epoch 83, loss : 0.0024, val_loss : 0.5540, acc : 0.944\n",
            "Epoch 84, loss : 0.0023, val_loss : 0.5488, acc : 0.944\n",
            "Epoch 85, loss : 0.0023, val_loss : 0.5460, acc : 0.944\n",
            "Epoch 86, loss : 0.0022, val_loss : 0.5448, acc : 0.944\n",
            "Epoch 87, loss : 0.0022, val_loss : 0.5456, acc : 0.944\n",
            "Epoch 88, loss : 0.0022, val_loss : 0.5465, acc : 0.944\n",
            "Epoch 89, loss : 0.0022, val_loss : 0.5459, acc : 0.944\n",
            "Epoch 90, loss : 0.0021, val_loss : 0.5448, acc : 0.944\n",
            "Epoch 91, loss : 0.0021, val_loss : 0.5430, acc : 0.944\n",
            "Epoch 92, loss : 0.0021, val_loss : 0.5427, acc : 0.944\n",
            "Epoch 93, loss : 0.0020, val_loss : 0.5434, acc : 0.944\n",
            "Epoch 94, loss : 0.0020, val_loss : 0.5436, acc : 0.944\n",
            "Epoch 95, loss : 0.0020, val_loss : 0.5429, acc : 0.944\n",
            "Epoch 96, loss : 0.0020, val_loss : 0.5419, acc : 0.944\n",
            "Epoch 97, loss : 0.0020, val_loss : 0.5412, acc : 0.944\n",
            "Epoch 98, loss : 0.0019, val_loss : 0.5408, acc : 0.944\n",
            "Epoch 99, loss : 0.0019, val_loss : 0.5407, acc : 0.944\n",
            "test_acc : 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRz2cjpNnHy9"
      },
      "source": [
        "<h2>[Problem 4] Creating a model of House Prices</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R01Azsj6nJbx",
        "outputId": "3d9a72ea-de3b-4db0-e84f-cb724f6038f6"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Load dataset\n",
        "df = pd.read_csv(house_path)\n",
        "df[\"SalePrice\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "140000    20\n",
              "135000    17\n",
              "145000    14\n",
              "155000    14\n",
              "190000    13\n",
              "          ..\n",
              "84900      1\n",
              "424870     1\n",
              "415298     1\n",
              "62383      1\n",
              "34900      1\n",
              "Name: SalePrice, Length: 663, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "k2iUGTr7n2i_",
        "outputId": "4ff557c0-10fa-4940-9659-cc9dbbf1aceb"
      },
      "source": [
        "#Condition extraction from data frame\n",
        "y = df[\"SalePrice\"]\n",
        "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
        "display(y.shape)\n",
        "display(X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(1460,)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(1460, 2)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "6im_mxYAoYrM",
        "outputId": "7bfe2532-3bfc-48fa-e809-acf1f3f44719"
      },
      "source": [
        "# NumPy 配列に変換\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "y = y.astype(np.int64)\n",
        "display(y.shape)\n",
        "display(y[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(1460,)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([208500, 181500, 223500, 140000, 250000])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_X0PRYBolnn"
      },
      "source": [
        "#Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "V1P4QnWW5qRx",
        "outputId": "da77527a-1834-4017-caea-4ae001a00c32"
      },
      "source": [
        "#do standard scaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train = scaler.transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "display(X_train[:5, :])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[-0.80043595, -1.318735  ],\n",
              "       [ 0.87578699, -1.93738445],\n",
              "       [-0.02107061,  1.22098379],\n",
              "       [ 0.28049286, -1.18849301],\n",
              "       [-0.16989415,  0.96049981]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ilcu0V2rW1o",
        "outputId": "54e9ee04-a042-4e75-b396-0bc3aef0542c"
      },
      "source": [
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None])\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weight and bias\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    return layer_output\n",
        "#Read network structure                              \n",
        "logits = example_net(X)\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.keras.losses.mean_squared_error(Y, logits))\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Estimated result\n",
        "#correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(logits - 0.5))\n",
        "#Indicator value calculation\n",
        "#accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#Initialization of variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        #total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, total_loss, val_loss))\n",
        "    test_err = sess.run(loss_op, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_mean_square_error : {:.3f}\".format(test_err))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 3948120381.9443, val_loss : 37120552960.0000\n",
            "Epoch 1, loss : 3942776108.4026, val_loss : 37057601536.0000\n",
            "Epoch 2, loss : 3934423912.7024, val_loss : 36954574848.0000\n",
            "Epoch 3, loss : 3920518896.1028, val_loss : 36782608384.0000\n",
            "Epoch 4, loss : 3897984047.1435, val_loss : 36513767424.0000\n",
            "Epoch 5, loss : 3864252709.8244, val_loss : 36124147712.0000\n",
            "Epoch 6, loss : 3816531318.9550, val_loss : 35584675840.0000\n",
            "Epoch 7, loss : 3752163932.0942, val_loss : 34875674624.0000\n",
            "Epoch 8, loss : 3669608737.4390, val_loss : 33987112960.0000\n",
            "Epoch 9, loss : 3568014139.7516, val_loss : 32912873472.0000\n",
            "Epoch 10, loss : 3447110648.3255, val_loss : 31655790592.0000\n",
            "Epoch 11, loss : 3307708958.6981, val_loss : 30229235712.0000\n",
            "Epoch 12, loss : 3151456034.5353, val_loss : 28652154880.0000\n",
            "Epoch 13, loss : 2980598980.2484, val_loss : 26949185536.0000\n",
            "Epoch 14, loss : 2797911360.1370, val_loss : 25149718528.0000\n",
            "Epoch 15, loss : 2606559169.5075, val_loss : 23286319104.0000\n",
            "Epoch 16, loss : 2409888426.4839, val_loss : 21391998976.0000\n",
            "Epoch 17, loss : 2211511533.9101, val_loss : 19505090560.0000\n",
            "Epoch 18, loss : 2015554831.8972, val_loss : 17665513472.0000\n",
            "Epoch 19, loss : 1825969613.2934, val_loss : 15909186560.0000\n",
            "Epoch 20, loss : 1646339573.5846, val_loss : 14267968512.0000\n",
            "Epoch 21, loss : 1479771349.2420, val_loss : 12767963136.0000\n",
            "Epoch 22, loss : 1328715976.3597, val_loss : 11428407296.0000\n",
            "Epoch 23, loss : 1194856647.9486, val_loss : 10259883008.0000\n",
            "Epoch 24, loss : 1079018677.4475, val_loss : 9265609728.0000\n",
            "Epoch 25, loss : 981301615.0064, val_loss : 8441699840.0000\n",
            "Epoch 26, loss : 901037723.2719, val_loss : 7777537536.0000\n",
            "Epoch 27, loss : 836915978.2099, val_loss : 7257112064.0000\n",
            "Epoch 28, loss : 787122958.8694, val_loss : 6860765696.0000\n",
            "Epoch 29, loss : 749509594.7923, val_loss : 6567283200.0000\n",
            "Epoch 30, loss : 721860862.3555, val_loss : 6355592704.0000\n",
            "Epoch 31, loss : 702034955.3062, val_loss : 6206364672.0000\n",
            "Epoch 32, loss : 688093361.6788, val_loss : 6102720512.0000\n",
            "Epoch 33, loss : 678416923.8201, val_loss : 6031108608.0000\n",
            "Epoch 34, loss : 671714864.6510, val_loss : 5981198336.0000\n",
            "Epoch 35, loss : 667023186.6381, val_loss : 5945603072.0000\n",
            "Epoch 36, loss : 663655108.3169, val_loss : 5919194112.0000\n",
            "Epoch 37, loss : 661144573.9443, val_loss : 5898617344.0000\n",
            "Epoch 38, loss : 659182257.6103, val_loss : 5881745408.0000\n",
            "Epoch 39, loss : 657569413.3448, val_loss : 5867311104.0000\n",
            "Epoch 40, loss : 656186118.5096, val_loss : 5854593024.0000\n",
            "Epoch 41, loss : 654956015.2805, val_loss : 5843039744.0000\n",
            "Epoch 42, loss : 653830709.3790, val_loss : 5832467456.0000\n",
            "Epoch 43, loss : 652781047.4347, val_loss : 5822621184.0000\n",
            "Epoch 44, loss : 651788971.1692, val_loss : 5813340672.0000\n",
            "Epoch 45, loss : 650847345.3362, val_loss : 5804476416.0000\n",
            "Epoch 46, loss : 649949835.2377, val_loss : 5796206080.0000\n",
            "Epoch 47, loss : 649093608.1542, val_loss : 5788216320.0000\n",
            "Epoch 48, loss : 648274291.8030, val_loss : 5780787712.0000\n",
            "Epoch 49, loss : 647487126.8865, val_loss : 5773556736.0000\n",
            "Epoch 50, loss : 646731006.3555, val_loss : 5766760960.0000\n",
            "Epoch 51, loss : 646002825.4561, val_loss : 5760299008.0000\n",
            "Epoch 52, loss : 645295814.9893, val_loss : 5753577472.0000\n",
            "Epoch 53, loss : 644623123.3233, val_loss : 5747432448.0000\n",
            "Epoch 54, loss : 643975174.9893, val_loss : 5741572096.0000\n",
            "Epoch 55, loss : 643349863.6060, val_loss : 5735861248.0000\n",
            "Epoch 56, loss : 642748330.3469, val_loss : 5730633216.0000\n",
            "Epoch 57, loss : 642167022.4582, val_loss : 5725474304.0000\n",
            "Epoch 58, loss : 641606382.0471, val_loss : 5720570368.0000\n",
            "Epoch 59, loss : 641062757.2762, val_loss : 5715739648.0000\n",
            "Epoch 60, loss : 640542688.6167, val_loss : 5711227392.0000\n",
            "Epoch 61, loss : 640044791.0921, val_loss : 5706828800.0000\n",
            "Epoch 62, loss : 639567282.8437, val_loss : 5702350336.0000\n",
            "Epoch 63, loss : 639109460.2827, val_loss : 5698192384.0000\n",
            "Epoch 64, loss : 638668055.7088, val_loss : 5694220800.0000\n",
            "Epoch 65, loss : 638240557.9101, val_loss : 5690286592.0000\n",
            "Epoch 66, loss : 637828316.3683, val_loss : 5686599168.0000\n",
            "Epoch 67, loss : 637427231.5203, val_loss : 5683035136.0000\n",
            "Epoch 68, loss : 637040221.3276, val_loss : 5679486976.0000\n",
            "Epoch 69, loss : 636664860.9165, val_loss : 5676140544.0000\n",
            "Epoch 70, loss : 636298784.4797, val_loss : 5672776704.0000\n",
            "Epoch 71, loss : 635951622.9893, val_loss : 5669667840.0000\n",
            "Epoch 72, loss : 635616127.8630, val_loss : 5666564096.0000\n",
            "Epoch 73, loss : 635296981.5161, val_loss : 5663703040.0000\n",
            "Epoch 74, loss : 634990083.4261, val_loss : 5660872704.0000\n",
            "Epoch 75, loss : 634699493.4133, val_loss : 5658256896.0000\n",
            "Epoch 76, loss : 634418928.6510, val_loss : 5655618048.0000\n",
            "Epoch 77, loss : 634151791.8287, val_loss : 5653190144.0000\n",
            "Epoch 78, loss : 633892904.8394, val_loss : 5650740736.0000\n",
            "Epoch 79, loss : 633646581.4475, val_loss : 5648489984.0000\n",
            "Epoch 80, loss : 633406553.6274, val_loss : 5646196736.0000\n",
            "Epoch 81, loss : 633177204.6253, val_loss : 5644097024.0000\n",
            "Epoch 82, loss : 632954354.7066, val_loss : 5641933312.0000\n",
            "Epoch 83, loss : 632739613.6017, val_loss : 5639921152.0000\n",
            "Epoch 84, loss : 632530143.2463, val_loss : 5637861888.0000\n",
            "Epoch 85, loss : 632329613.4304, val_loss : 5635971584.0000\n",
            "Epoch 86, loss : 632135127.1606, val_loss : 5634063360.0000\n",
            "Epoch 87, loss : 631949985.7131, val_loss : 5632354816.0000\n",
            "Epoch 88, loss : 631770477.2248, val_loss : 5630688768.0000\n",
            "Epoch 89, loss : 631598254.8694, val_loss : 5629090304.0000\n",
            "Epoch 90, loss : 631433614.3897, val_loss : 5627462656.0000\n",
            "Epoch 91, loss : 631276087.2291, val_loss : 5625897472.0000\n",
            "Epoch 92, loss : 631127081.2505, val_loss : 5624518144.0000\n",
            "Epoch 93, loss : 630981290.0728, val_loss : 5623173632.0000\n",
            "Epoch 94, loss : 630842466.9465, val_loss : 5621876224.0000\n",
            "Epoch 95, loss : 630709165.9101, val_loss : 5620541440.0000\n",
            "Epoch 96, loss : 630580925.8073, val_loss : 5619259392.0000\n",
            "Epoch 97, loss : 630458111.1777, val_loss : 5618121216.0000\n",
            "Epoch 98, loss : 630337558.6124, val_loss : 5616921600.0000\n",
            "Epoch 99, loss : 630221680.9251, val_loss : 5615828480.0000\n",
            "test_mean_square_error : 7162448384.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izIu6Yhi0S8y"
      },
      "source": [
        "The difference between classification and regression implementation of NN:\n",
        "<ul>\n",
        "<li>\n",
        "    Classification:\n",
        "    <ul>\n",
        "    <li>Using softmax activation function to output value based on probability</li>\n",
        "    <li>Using cross entropy loss function</li>\n",
        "    </ul>\n",
        "</li>\n",
        "\n",
        "<li>\n",
        "    Regression:\n",
        "    <ul>\n",
        "    <li>Using linear function to regress a final value</li>\n",
        "    <li>Using mean square error loss function</li>\n",
        "    </ul>\n",
        "</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Yk-XuIf1eCI"
      },
      "source": [
        "<h2>[Problem 5] Creating a MNIST model</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTUbWltC1cTm"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "kAOlaxl2xm4n",
        "outputId": "deca58e4-93bc-4dd9-e053-38b57691ec4e"
      },
      "source": [
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "display(X_train.shape)\n",
        "display(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enmoNNW_x5ao"
      },
      "source": [
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "zx688lDtyBVg",
        "outputId": "20ff19e4-cb12-40a4-e6e7-e87d941732b7"
      },
      "source": [
        "# Convert label to one hot\n",
        "y_train = pd.get_dummies(y_train)\n",
        "y_test = pd.get_dummies(y_test)\n",
        "\n",
        "# NumPy 配列に変換\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "y_train = y_train.astype(np.int64)\n",
        "y_test = y_test.astype(np.int64)\n",
        "\n",
        "display(y_train.shape)\n",
        "display(y_test.shape)\n",
        "\n",
        "display(y_train[:5, :])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(10000, 10)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdOV9TbPzAvO"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bDRgJFRd6yl"
      },
      "source": [
        "<h3>Using FC only</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svfjDnGbzdQk",
        "outputId": "4765003c-4c6f-4439-cb69-7e9c1d6cb7e5"
      },
      "source": [
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 20\n",
        "num_epochs = 10\n",
        "n_hidden1 = 400\n",
        "n_hidden2 = 200\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weight and bias\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    return layer_output\n",
        "#Read network structure                              \n",
        "logits = example_net(X)\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.nn.softmax(logits) - 0.5))\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#Initialization of variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 4.9512, val_loss : 34.6165, acc : 0.981\n",
            "Epoch 1, loss : 1.0398, val_loss : 22.8223, acc : 0.986\n",
            "Epoch 2, loss : 0.4929, val_loss : 18.4501, acc : 0.987\n",
            "Epoch 3, loss : 0.2546, val_loss : 15.4491, acc : 0.988\n",
            "Epoch 4, loss : 0.1417, val_loss : 14.8346, acc : 0.988\n",
            "Epoch 5, loss : 0.0819, val_loss : 14.8641, acc : 0.989\n",
            "Epoch 6, loss : 0.0492, val_loss : 12.5961, acc : 0.990\n",
            "Epoch 7, loss : 0.0360, val_loss : 12.5920, acc : 0.991\n",
            "Epoch 8, loss : 0.0265, val_loss : 12.4742, acc : 0.991\n",
            "Epoch 9, loss : 0.0213, val_loss : 12.0821, acc : 0.992\n",
            "test_acc : 0.992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JACi2w94eAwp"
      },
      "source": [
        "<h3>Using CNN</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXwiTtKyeC9d",
        "outputId": "c5456906-e98d-42b9-e6da-eff3b2d28999"
      },
      "source": [
        "# Hyperparameter settings\n",
        "learning_rate = 0.01\n",
        "batch_size = 200\n",
        "num_epochs = 30\n",
        "n_hidden1 = 120\n",
        "n_hidden2 = 84\n",
        "n_input = X_train.shape[-1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def conv_net(x):\n",
        "    tf.random.set_random_seed(1)\n",
        "\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([5, 5, 1, 6])),\n",
        "        'w2': tf.Variable(tf.random_normal([5, 5, 6, 16])),\n",
        "    }\n",
        "\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([6])),\n",
        "        'b2': tf.Variable(tf.random_normal([16])),\n",
        "    }\n",
        "\n",
        "    x = tf.reshape(x, [-1, 28, 28, 1]) #1@28x28\n",
        "\n",
        "    c1 = tf.add(tf.nn.conv2d(x, weights['w1'], [1,1,1,1], padding=\"VALID\"), biases['b1']) #6@24x24\n",
        "    c1 = tf.nn.relu(c1) #6@24x24\n",
        "\n",
        "    s2 = tf.nn.pool(c1, window_shape=[2,2], pooling_type=\"MAX\", strides=[2,2], padding=\"VALID\") #6@12x12\n",
        "\n",
        "    c3 = tf.add(tf.nn.conv2d(s2, weights['w2'], [1,1,1,1], padding=\"VALID\"), biases['b2']) #16@8x8\n",
        "    c3 = tf.nn.relu(c3) #16@10x10\n",
        "\n",
        "    s4 = tf.nn.pool(c3, window_shape=[2,2], pooling_type=\"MAX\", strides=[2,2], padding=\"VALID\") #16@4x4\n",
        "    #flattening\n",
        "    layer_output = tf.reshape(s4, [-1, 16*4*4])\n",
        "    return layer_output\n",
        "\n",
        "def fc_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weight and bias\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([x.shape[-1], n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    return layer_output\n",
        "\n",
        "def le_net(x):\n",
        "    return fc_net(conv_net(x))\n",
        "\n",
        "#Read network structure                              \n",
        "logits = le_net(X)\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.nn.softmax(logits) - 0.5))\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#Initialization of variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, val_acc : {:.3f}\".format(epoch, total_loss, val_loss, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 3.1401, val_loss : 107.8022, val_acc : 0.974\n",
            "Epoch 1, loss : 0.3384, val_loss : 53.8764, val_acc : 0.981\n",
            "Epoch 2, loss : 0.1725, val_loss : 37.3705, val_acc : 0.985\n",
            "Epoch 3, loss : 0.1092, val_loss : 29.6584, val_acc : 0.986\n",
            "Epoch 4, loss : 0.0751, val_loss : 24.8350, val_acc : 0.987\n",
            "Epoch 5, loss : 0.0536, val_loss : 21.1964, val_acc : 0.989\n",
            "Epoch 6, loss : 0.0409, val_loss : 19.9643, val_acc : 0.989\n",
            "Epoch 7, loss : 0.0304, val_loss : 17.6325, val_acc : 0.990\n",
            "Epoch 8, loss : 0.0248, val_loss : 15.9675, val_acc : 0.990\n",
            "Epoch 9, loss : 0.0188, val_loss : 15.6557, val_acc : 0.990\n",
            "Epoch 10, loss : 0.0160, val_loss : 14.8198, val_acc : 0.990\n",
            "Epoch 11, loss : 0.0137, val_loss : 14.2430, val_acc : 0.990\n",
            "Epoch 12, loss : 0.0105, val_loss : 13.6983, val_acc : 0.990\n",
            "Epoch 13, loss : 0.0105, val_loss : 14.5615, val_acc : 0.990\n",
            "Epoch 14, loss : 0.0093, val_loss : 13.4303, val_acc : 0.991\n",
            "Epoch 15, loss : 0.0080, val_loss : 11.8737, val_acc : 0.991\n",
            "Epoch 16, loss : 0.0069, val_loss : 11.3130, val_acc : 0.992\n",
            "Epoch 17, loss : 0.0076, val_loss : 9.8644, val_acc : 0.992\n",
            "Epoch 18, loss : 0.0065, val_loss : 10.0331, val_acc : 0.993\n",
            "Epoch 19, loss : 0.0070, val_loss : 9.4372, val_acc : 0.993\n",
            "Epoch 20, loss : 0.0049, val_loss : 7.9928, val_acc : 0.993\n",
            "Epoch 21, loss : 0.0046, val_loss : 9.0845, val_acc : 0.993\n",
            "Epoch 22, loss : 0.0056, val_loss : 10.8359, val_acc : 0.992\n",
            "Epoch 23, loss : 0.0050, val_loss : 7.5102, val_acc : 0.993\n",
            "Epoch 24, loss : 0.0044, val_loss : 7.1892, val_acc : 0.994\n",
            "Epoch 25, loss : 0.0034, val_loss : 7.7353, val_acc : 0.992\n",
            "Epoch 26, loss : 0.0040, val_loss : 7.5103, val_acc : 0.993\n",
            "Epoch 27, loss : 0.0045, val_loss : 7.1033, val_acc : 0.994\n",
            "Epoch 28, loss : 0.0032, val_loss : 6.7699, val_acc : 0.993\n",
            "Epoch 29, loss : 0.0031, val_loss : 6.0044, val_acc : 0.993\n",
            "test_acc : 0.994\n"
          ]
        }
      ]
    }
  ]
}